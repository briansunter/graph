---
date: 2022-11-26
categories:
- programming
- logseq
tags:
- ai
- programming
- gpt3
- large language models
description: Large language models have a number of interesting properties
coverimage: /assets/Screen_Shot_2022-09-11_at_8.35.17_PM_1668815306606_0.png
title: why-large-language-models-are-interesting
lastMod: 2022-12-07
---
I continue to be really interested in this new AI technology called "Large Language Models" (LLMs)

Large language models are AI tools that can perform a wide variety of tasks. They can be used for both search and classification based on the meaning of text, as well as generating humanlike text.

They were trained by analyzing massive amounts of text from places like Wikipedia and Reddit.

I think they are interesting for three reasons:

### Natural Language Interface

First, you can interact with the text generation capabilities using natural language, instead of with code.

For example, you can ask it "Translate this sentence into French:" or "Summarize this news article:".

This is a new way of coding, where you can use English language to perform useful tasks instead of writing code.

There's interesting research being done in this area, such as Google's "PromptChainer", which lets you build AI enabled apps by chaining together these english commands using a visual interface.

![FhuU9ZRUUAEXf9e.jpeg](/assets/FhuU9ZRUUAEXf9e_1669150904874_0.jpeg)

AI enabled "no code" interfaces like this could enable non-programmers to perform useful tasks and create powerful applications.

### Generality and Emergence

Second, LLMs extremely general and can perform a wide variety of tasks, even things they weren't originally trained to do. Earlier AI models could only perform tasks they were specifically trained to do, like summarization or translation. GPT-3 can do things like multiply numbers, even though it wasn't trained to do it.

These LLMs seem to get better and gain new abilities as they are trained on more text. So as you feed them more data, they become more accurate and can do new types of tasks.

This behavior is called "emergent abilities", where as a model becomes larger, it can do things smaller models can't do.

For example, the ability to perform tasks like arithmetic and answering college entrance exam questions only "emerge" after you feed the AI a certain amount of data.

![Screen Shot 2022-09-11 at 8.35.17 PM.png](/assets/Screen_Shot_2022-09-11_at_8.35.17_PM_1668815306606_0.png)

This opens up the interesting possibility that just by feeding AI more data, it can gain new  abilities unforeseen by its creators. The recent advancements in computing performance allow us to generate AI models that are much, much larger than before.

We're seeing similar behavior in the area of "reinforcement learning" like Google's Alpha Zero Go board game engine. Instead of experts building in strategies for board games, like a database of optimal opening and endgame moves, these systems teach themselves by playing millions of games against themselves. This allows them to come up with strategies the creators couldn't have imagined.

### Semantic Search and Classification

Third, in addition to generating text, these models can also used for search and classification. Instead of just searching for words that appear in text, they can find text with similar meaning.

This "semantic search" technology is one of the reasons Google search is so good, and can find what you're looking for even if the exact phrase doesn't appear in the text. Until recently, this tech was only available inside large tech companies, but it's finally becoming more accessible.

Since the AI understands the meaning of text, it can also be used to group texts and discover hidden relationships between them.


